---
title: "Logistic Elastic Net regression model on Differential Expression Species"
date: "Created: 2021-11-10 Updated: `r Sys.Date()`"
author: 
  - name: "Hua Zou"
    email: "zouhua1@outlook.com"
output: 
  html_notebook:
    codes: hide
---

### Introdution

Building Elastic Net regression model based on the Differential Expression Proteins.


* Elastic-regularized logistic regression trained on CRC vs non-CRC (AA/HC),
    
* Elastic-regularized logistic regression trained on CRC vs non-CRC (AA/HC), allowing positive weights
    
* Elastic-regularized logistic regression trained on CRC vs non-CRC, allowing negative weights
    
**Notes: The elastic net penalty is controlled by α, and bridges the gap between Elastic regression (α=1, the default) and ridge regression (α=0). The tuning parameter λ controls the overall strength of the penalty.**

=======================================================================

DataStatus: 

  * Healthy Control: HC
  
  * Adenoma: AA	
  
  * Colorectal Cancer: CRC

* Discovery Cohort: 1112

  * 626 non-CRC samples (481 HC + 145 AA)
  
  * 486 CRC samples

* Validation Cohort: 475 

  * 268 non-CRC samples(206 HC + 62 AA)
  
  * 207 CRC samples
  
=======================================================================

Four comparisons between the Colorectal Cancer (CRC) patients and other groups:

  1. CRC 486 vs HC 481
  
  2. CRC 486 vs AA 145
  
  3. AA 145 vs HC 481
  
  4. **CRC 486 vs non-CRC( 481 HC; 145 AA) 626**
  
========================================================================

General Evaluation of model performance

* AUC

* Accuracy

* Sensitivity

* Specificity 

* Positive Predictive Value(Precision)

========================================================================



```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = F, message = F)
library(dplyr)
library(tibble)
library(convert)
library(data.table)
library(caret)
library(glmnet)
library(ggplot2)

# rm(list = ls())
options(stringsAsFactors = F)
options(future.globals.maxSize = 1000 * 1024^2)

grp <- c("control", "adenoma", "CRC")
subgrp <- c("HC", "AA", "CRC")
grp.col <- c("#568875", "#73FAFC", "#EE853D")
```


### Importing Data 
```{r}
ExprSet <- readRDS("../../Result/Biomarker/Logistic_model/species_profile_Discovery.RDS")
ExprSet_Val <- readRDS("../../Result/Biomarker/Logistic_model/species_profile_Validation.RDS")

CRC_HC_DEA <- fread("../../Result/Biomarker/Logistic_model/CRC_HC_DEA_final.csv")
CRC_nonCRC_DEA <- fread("../../Result/Biomarker/Logistic_model/CRC_nonCRC_DEA_final.csv")
```


### logistic Elastic regression model
```{r}
build_Elastic <- function(dataset=ExprSet,
                          testres=CRC_HC_DEA,
                          group_name=subgrp[c(1,3)],
                          Pname="P.value",
                          Pval=0.05,
                          LogFC=0,
                          splitPro=0.5,
                          kfold=10,
                          times=500){
  
  # dataset=ExprSet
  # testres=CRC_HC_DEA
  # group_name=subgrp[c(1,3)]
  # Pname="P.value"
  # Pval=0.05
  # LogFC=0
  # splitPro=0.5
  # kfold=10
  # times=5
  
  # Differential Proteins
  colnames(testres)[which(colnames(testres) == Pname)] <- "PValue"
  if(Pval == 0){
    dat_DEP <- testres %>% dplyr::select(FeatureID, PValue)
  }else{
    if(LogFC > 0){
      dat_DEP <- testres %>% dplyr::select(FeatureID, PValue, logFC) %>%
        filter(PValue < Pval) %>%
        filter(abs(logFC) > LogFC)    
    }else{
      dat_DEP <- testres %>% dplyr::select(FeatureID, PValue) %>%
        filter(PValue < Pval)    
    }  
  }
  print(dim(dat_DEP)[1])
  
  # dataset of DEP
  if(length(group_name) == 3){
    group_name2 <- c("non_CRC", "CRC")
    pheno <- pData(dataset) %>%
      filter(Group%in%group_name) %>%
      mutate(Group=ifelse(Group == group_name2[2], group_name2[2], group_name2[1])) %>%
      mutate(Group=factor(Group, levels = group_name2))
    group_name <- group_name2
  }else{
    pheno <- pData(dataset) %>%
      filter(Group%in%group_name) %>%
      mutate(Group=factor(Group, levels = group_name))    
  }
  edata <- exprs(dataset)[dat_DEP$FeatureID, rownames(pheno)] %>% data.frame()
  
  # mdat without CA199
  mdat_temp <- inner_join(pheno %>% rownames_to_column("SampleID") %>%
                         dplyr::select(SampleID, Group),
                       edata %>% t() %>% data.frame() %>%
                         rownames_to_column("SampleID"),
                       by = "SampleID") %>%
      column_to_rownames("SampleID")
  
  # Before building model, performing Zscore standardization
  mdat <- cbind(mdat_temp$Group, scale(mdat_temp %>% dplyr::select(-Group)) %>% data.frame())
  colnames(mdat)[1] <- "Group"  
  
  # Data Partition
  if(!is.null(splitPro)){
    # set.seed(123)
    require(sampling)
    sample <- sampling::strata(mdat, 
                       stratanames = "Group", 
                       size = round(as.numeric(table(mdat$Group)) * splitPro),
                       method = "srswor")
    trainData <- mdat[sample$ID_unit, ]
    testData <- mdat[-sample$ID_unit, ]
  }else{
    trainData <- mdat
    testData <- mdat
  }
  
  dat_table <- as.matrix(trainData %>% dplyr::select(-Group))
  dat_target <- trainData$Group
    
  print("The Group's number of TrainData")    
  print(table(trainData$Group)) 
  
  # preview data distribution
  # plot(density(dat_table))
    
  # determine folds
  if(dim(trainData)[1] > 10 * 8){
    kfold <- 10
  }else{
    kfold <- kfold
  }
  
  # tuning parameters
  lambdas_alpha <- c()
  for(i in 1:times){
    model <- caret::train(Group ~., data = trainData, 
                          method = "glmnet",
                          trControl = trainControl("cv", number = kfold),
                          tuneLength = 10)
    errors <- data.frame(lambda=as.numeric(model$bestTune)[2], 
                         alpha=as.numeric(model$bestTune)[1])
    lambdas_alpha <- rbind(lambdas_alpha, errors)
  }
  
  # Regard median lambda & alpha as best lambda & alpha
  bestlambda <- median(lambdas_alpha$lambda)
  bestalpha <- median(lambdas_alpha$alpha)
  bestfit_all <- glmnet(x=dat_table, 
                        y=dat_target, 
                        family='binomial',
                        alpha = bestalpha,
                        lambda = bestlambda,
                        type.measure = "auc")
  predicted_all <- predict(bestfit_all, as.matrix(testData[, -1]), type = "response", s = bestlambda)
  
  # evaluate the performance of model
  predict_class <- predict(bestfit_all, as.matrix(testData[, -1]), type = "class")
  print(caret::confusionMatrix(factor(predict_class, levels = group_name), 
                               factor(testData$Group, levels = group_name)))
  
  # AUC
  require(pROC)
  rocobj_all <- roc(testData$Group, predictor=predicted_all[, 1])
  
  # AUC 95% CI 
  auc_95CI <- ci.auc(rocobj_all, conf.level = 0.95)
  auc_95CI_value <- round(as.numeric(auc_95CI), 3)
  auc_95CI_label <- paste0("AUC = ", auc_95CI_value[1], " (", 
                            auc_95CI_value[2], ", ", 
                           auc_95CI_value[3], ")")  
    
  auc_all <- round(auc(testData$Group, predictor=predicted_all[, 1]), 4)
  pl_all <- ggroc(rocobj_all, color = "red", linetype = 1, size = 1, alpha = 1, legacy.axes = T)+
                  geom_abline(intercept = 0, slope = 1, color="grey", size = 1, linetype=1)+
                  labs(x = "False Positive Rate (1 - Specificity)",
                       y = "True Positive Rate (Sensivity or Recall)")+
                  # annotate("text",x = .75, y = .25, label = paste("AUC =", auc_all),
                  #          size = 5, family = "serif")+
                  annotate("text",x = .75, y = .25, label = auc_95CI_label,
                           size = 5, family = "serif")+    
                  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))+
                  theme_bw()+
                  theme(panel.background = element_rect(fill = 'transparent'),
                        axis.ticks.length = unit(0.4, "lines"), 
                        axis.ticks = element_line(color='black'),
                        axis.line = element_line(size=.5, colour = "black"),
                        axis.title = element_text(colour='black', size=12,face = "bold"),
                        axis.text = element_text(colour='black',size=10),
                        text = element_text(size=8, color="black", family="serif"))  
  
  # predictors
  predictors_all <- data.frame(as.matrix(coef(bestfit_all, bestlambda))) %>%
        setNames("Score") %>%
        rownames_to_column("Protein") %>%
        slice(-1) %>%
        filter(Score!=0) %>%
        mutate(Director=ifelse(Score > 0, "Positive", "Negative"))%>%
        arrange(Director, desc(Score))  
  
  
  # Positive or negative with CA199
  coefficient_plot <- function(datpred=predictors_all,
                               trainSet=trainData,
                               testSet=testData,
                               Lambda=bestlambda,
                               Alpha=bestalpha,
                               tag="Positive"){
  
    # datpred=predictors_all
    # trainSet=trainData
    # testSet=testData
    # Lambda=bestlambda 
    # Alpha=bestalpha
    # tag="Positive"
    
    pred_cln <- datpred %>% filter(Director%in%tag)
    trainSet_cln <- trainSet %>% dplyr::select(all_of(c("Group", pred_cln$Protein)))
    testSet_cln <- testSet %>% dplyr::select(all_of(c("Group", pred_cln$Protein))) 
    pred_final <- pred_cln
    dat_table_cln <- as.matrix(trainSet_cln %>% dplyr::select(-Group))
    dat_target_cln <- trainSet_cln$Group
    
    if(ncol(trainSet_cln) <= 2){
      return(list(fit=NA, roc=NA, auc=NA, pl=NA, marker=NA))
    }    
    
    bestfit_cln <- glmnet(x=dat_table_cln, 
                          y=dat_target_cln, 
                          family='binomial',
                          alpha=Alpha,
                          lambda=Lambda,
                          type.measure="auc")
    predicted_res <- predict(bestfit_cln, as.matrix(testSet_cln[, -1]), 
                             type = "response", s = Lambda)
    rocobj_res <- roc(testSet_cln$Group, predictor=predicted_res[, 1])
    
    # AUC 95% CI 
    auc_95CI <- ci.auc(rocobj_res, conf.level = 0.95)
    auc_95CI_value <- round(as.numeric(auc_95CI), 3)
    auc_95CI_label <- paste0("AUC = ", auc_95CI_value[1], " (", 
                             auc_95CI_value[2], ", ", 
                             auc_95CI_value[3], ")") 
  
    auc_res <- round(auc(testSet_cln$Group, predictor=predicted_res[, 1]), 4)
    pl_res <- ggroc(rocobj_res, color = "red", linetype = 1, size = 1, alpha = 1, legacy.axes = T)+
                      geom_abline(intercept = 0, slope = 1, color="grey", size = 1, linetype=1)+
                    labs(x = "False Positive Rate (1 - Specificity)",
                         y = "True Positive Rate (Sensivity or Recall)")+
                    # annotate("text",x = .75, y = .25, label = paste("AUC =", auc_res),
                    #          size = 5, family = "serif")+
                    annotate("text",x = .75, y = .25, label = auc_95CI_label,
                             size = 5, family = "serif")+      
                    coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))+
                    theme_bw()+
                    theme(panel.background = element_rect(fill = 'transparent'),
                          axis.ticks.length = unit(0.4, "lines"), 
                          axis.ticks = element_line(color='black'),
                          axis.line = element_line(size=.5, colour = "black"),
                          axis.title = element_text(colour='black', size=12,face = "bold"),
                          axis.text = element_text(colour='black',size=10),
                          text = element_text(size=8, color="black", family="serif"))
    
    res <- list(fit=bestfit_cln,
                roc=rocobj_res,
                auc=auc_res,
                auc_CI=auc_95CI_label,
                pl=pl_res,
                marker=pred_final)
    
    return(res)
  }
  
  # positive predictors
  pos_res <- coefficient_plot(trainSet=trainData,
                              testSet=testData,
                              tag="Positive")
  
  # negative predictors
  neg_res <- coefficient_plot(trainSet=trainData,
                              testSet=testData,
                              tag="Negative") 
 
  res <- list(lambda_alpha=lambdas_alpha,
              fit=list(AllFit=bestfit_all, 
                       PosFit=pos_res$fit, 
                       NegFit=neg_res$fit), 
              auc=list(AllAuc=auc_all, 
                       PosAuc=pos_res$auc,  
                       NegAuc=neg_res$auc),
              auc_CI=list(AllAucCI=auc_95CI_label, 
                       PosAucCI=pos_res$auc_CI, 
                       NegAucCI=neg_res$auc_CI),              
              roc=list(AllRoc=rocobj_all, 
                       PosRoc=pos_res$roc, 
                       NegRoc=neg_res$roc),
              pl=list(allPl=pl_all, 
                      PosPl=pos_res$pl,
                      NegPl=neg_res$pl),
              marker=list(allMarker=predictors_all, 
                      PosMarker=pos_res$marker,
                      NegMarker=neg_res$marker))  
  
  return(res)
}


model_validation <- function(dataset=ExprSet_Val,
                             datmarker=CRC_HC_DEA,
                             datfit=CRC_HC_LASSO$fit$AllFit,
                             group_name=subgrp[c(1,3)],
                             All=TRUE,
                             Pname="P.value",
                             Pval=0.05,
                             LogFC=0){

  # dataset=ExprSet_Val
  # datmarker=CRC_HC_DEA
  # datfit=CRC_HC_LASSO$fit$AllFit
  # group_name=subgrp[c(1,3)]
  # All=TRUE
  # Pname="P.value"
  # Pval=0.05
  # LogFC=0  

  # dataset of DEP
  if(length(group_name) == 3){
    group_name2 <- c("non_CRC", "CRC")
    pheno <- pData(dataset) %>%
      filter(Group%in%group_name) %>%
      mutate(Group=ifelse(Group == group_name2[2], group_name2[2], group_name2[1])) %>%
      mutate(Group=factor(Group, levels = group_name2))
    group_name <- group_name2
  }else{
    pheno <- pData(dataset) %>%
      filter(Group%in%group_name) %>%
      mutate(Group=factor(Group, levels = group_name))
  }

  profile <- exprs(dataset)
  
  if(All){
    # Differential Proteins
    colnames(datmarker)[which(colnames(datmarker) == Pname)] <- "PValue"
    if(Pval == 0){
      dat_DEP <- datmarker %>% dplyr::select(FeatureID, PValue)
    }else{
      if(LogFC > 0){
        dat_DEP <- datmarker %>% dplyr::select(FeatureID, PValue, logFC) %>%
          filter(PValue < Pval) %>%
          filter(abs(logFC) > LogFC)    
      }else{
        dat_DEP <- datmarker %>% dplyr::select(FeatureID, PValue) %>%
          filter(PValue < Pval)    
      }  
    }    
    
    edata <- profile[, colnames(profile)%in%rownames(pheno)] %>%
        data.frame() %>%
        rownames_to_column("FeatureID") %>%
        filter(FeatureID %in% dat_DEP$FeatureID) %>%
        column_to_rownames("FeatureID") 
  }else{
    edata <- profile[, colnames(profile)%in%rownames(pheno)] %>%
      data.frame() %>%
      rownames_to_column("FeatureID") %>%
      filter(FeatureID %in% datmarker) %>%
      column_to_rownames("FeatureID")    
  }
  # Data of whole feature
  mdat_temp <- inner_join(pheno %>% rownames_to_column("SampleID") %>%
                       dplyr::select(SampleID, Group),
                     edata %>% t() %>% data.frame() %>%
                       rownames_to_column("SampleID"),
                     by = "SampleID") %>%
    column_to_rownames("SampleID")
  
  # Before building model, performing Zscore standardization
  mdat <- cbind(mdat_temp$Group, scale(mdat_temp %>% dplyr::select(-Group)) %>% data.frame())
  colnames(mdat)[1] <- "Group" 
  

  # evaluate the performance of model
  predict_class <- predict(datfit, as.matrix(mdat[, -1]), type = "class")
  print(caret::confusionMatrix(factor(predict_class, levels = group_name), 
                               factor(mdat$Group, levels = group_name)))  

  pred_prob <- predict(datfit, as.matrix(mdat[, -1]), type = "response", s=datfit$lambda.min)

  # define roc and calculate AUC
  rocobj <- roc(mdat$Group, pred_prob[, 1])
  
  # AUC 95% CI 
  auc_95CI <- ci.auc(rocobj, conf.level = 0.95)
  auc_95CI_value <- round(as.numeric(auc_95CI), 3)
  auc_95CI_label <- paste0("AUC = ", auc_95CI_value[1], " (", 
                            auc_95CI_value[2], ", ", 
                           auc_95CI_value[3], ")")
  
  auc <- round(auc(mdat$Group, pred_prob[, 1]), 3)
  roc <- tibble(tpr=rocobj$sensitivities,
                fpr=1 - rocobj$specificities)
  pl <- ggplot(data=roc, aes(x=fpr, y=tpr))+
            geom_path(color="red", size=1)+
            geom_abline(intercept=0, slope=1, color="grey", size=1, linetype=2)+
            labs(x = "False Positive Rate (1 - Specificity)",
                 y = "True Positive Rate (Sensivity or Recall)")+
            # annotate("text", x=.75, y=.25, label=paste("AUC =", auc),
            #          size=5, family="serif")+
            annotate("text", x=.75, y=.25, label=auc_95CI_label,
                     size=5, family="serif")+    
            coord_cartesian(xlim=c(0, 1), ylim=c(0, 1))+
            theme_bw()+
            theme(panel.background = element_rect(fill="transparent"),
                  axis.ticks.length = unit(0.4, "lines"),
                  axis.ticks = element_line(color="black"),
                  axis.line = element_line(size=.5, color="black"),
                  axis.title = element_text(color='black', size=12, face="bold"),
                  axis.text = element_text(color='black', size=10),
                  text = element_text(size=8, color="black", family="serif"))

  # Accuracy
  if(length(unique(predict_class)) > 1){
    CFM <- caret::confusionMatrix(factor(predict_class, levels = group_name), 
                                  factor(mdat$Group, levels = group_name))
    CFM_index <- data.frame(Value=CFM$byClass) %>%
      rownames_to_column("Index") %>%
      rbind(data.frame(Index="Accuracy", Value=as.numeric(CFM$overall[1])))
  }else{
    CFM <- NA
    CFM_index <- NA
  }

  res <- list(roc=rocobj,
              auc=auc,
              auc_CI=auc_95CI_label,
              CFM=CFM,
              index=CFM_index,
              pl=pl)

  return(res)
}


model_performance <- function(rocobject=CRC_nonCRC_LASSO$roc$AllRoc,
                              auc_CI=CRC_nonCRC_LASSO$auc_CI$AllAucCI,
                              tag="CRC vs Non-CRC in All"){
  
  # rocobject=CRC_nonCRC_LASSO$roc$AllRoc
  # auc_CI=CRC_nonCRC_LASSO$auc_CI$AllAucCI
  # tag="CRC vs Non-CRC in All"
  
  require(pROC)
  # 95% CI 
  CI95_Normal <- function(x, tag){
    # Calculating a Confidence Interval From a Normal Distribution
    x_length <- length(x)
    x_mean <- signif(mean(x), 3)
    x_sd <- sd(x)
    error <- qnorm(0.975) * x_sd/sqrt(x_length)
    left <- signif(x_mean - error, 3)
    right <- signif(x_mean + error, 3)
    res <- paste0(x_mean, " (", left, ", ", right, ")")
    #res <- paste0(tag, " = ", x_mean, " (", left, ", ", right, ")")
    return(res)
  }
  
  if(is.na(rocobject)){
    return(data.frame(Name=tag, AUC=NA, Sensitivity=NA,
                    Specificity=NA, Precision=NA))
  }
  
  # Sensitivity and Specificity and positive predictive value
  ss <- coords(rocobject, "all", ret="sensitivity", transpose = TRUE)
  ss_95CI <- CI95_Normal(ss, "sensitivity")

  sf <- coords(rocobject, "all", ret="specificity", transpose = TRUE)
  sf_95CI <- CI95_Normal(sf, "specificity")

  PPV <- coords(rocobject, "all", ret="precision", transpose = TRUE)
  PPV_95CI <- CI95_Normal(PPV[!is.nan(PPV)], "precision")
  
  acc <- coords(rocobject, "all", ret="accuracy", transpose = TRUE)
  acc_95CI <- CI95_Normal(acc, "accuracy")  
  
  # AUC
  auc_95CI <- gsub("AUC = ", "", auc_CI) 
  
  res <- data.frame(Name=tag, AUC=auc_95CI, Accuracy=acc_95CI,
                    Sensitivity=ss_95CI, Specificity=sf_95CI, Precision=PPV_95CI)
  
  return(res)

}


if(!dir.exists("../../Result/Biomarker/Logistic_model")){
  dir.create("../../Result/Biomarker/Logistic_model", recursive = T)
}
```


### CRC vs HC 
```{r, fig.width=12, fig.height=5}
CRC_HC_Elastic <- build_Elastic(dataset=ExprSet,
                                testres=CRC_HC_DEA,
                                group_name=subgrp[c(1,3)],
                                Pname="P.value",
                                Pval=0.05,
                                LogFC=0,
                                splitPro=0.5,
                                kfold=10,
                                times=5)

CRC_HC_performance <- rbind(
    model_performance(rocobject=CRC_HC_Elastic$roc$AllRoc,
                  auc_CI=CRC_HC_Elastic$auc_CI$AllAucCI,
                  tag="HC vs CRC"),
    model_performance(rocobject=CRC_HC_Elastic$roc$PosRoc,
                  auc_CI=CRC_HC_Elastic$auc_CI$PosAucCI,
                  tag="HC vs CRC with positive weights"),
    model_performance(rocobject=CRC_HC_Elastic$roc$NegRoc,
                  auc_CI=CRC_HC_Elastic$auc_CI$NegAucCI,
                  tag="HC vs CRC with negative weights"))
DT::datatable(CRC_HC_performance)


CRC_HC_Val_all <- model_validation(dataset=ExprSet_Val,
                                   datmarker=CRC_HC_DEA,
                                   datfit=CRC_HC_Elastic$fit$AllFit,
                                   group_name=subgrp[c(1,3)],
                                   All=TRUE,
                                   Pname="P.value",
                                   Pval=0.05,
                                   LogFC=0)

CRC_HC_Val_Pos <- model_validation(dataset=ExprSet_Val,
                                   datmarker=CRC_HC_Elastic$marker$PosMarker$Protein,
                                   datfit=CRC_HC_Elastic$fit$PosFit,
                                   group_name=subgrp[c(1,3)],
                                   All=FALSE)
CRC_HC_Val_Neg <- model_validation(dataset=ExprSet_Val,
                                   datmarker=CRC_HC_Elastic$marker$NegMarker$Protein,
                                   datfit=CRC_HC_Elastic$fit$NegFit,
                                   group_name=subgrp[c(1,3)],
                                   All=FALSE)


CRC_HC_Val_performance <- rbind(
                model_performance(rocobject=CRC_HC_Val_all$roc,
                                  auc_CI=CRC_HC_Val_all$auc_CI,
                                  tag="HC vs CRC"),
                model_performance(rocobject=CRC_HC_Val_Pos$roc,
                                  auc_CI=CRC_HC_Val_Pos$auc_CI,
                                  tag="HC vs CRC with positive weights"),
                model_performance(rocobject=CRC_HC_Val_Neg$roc,
                                  auc_CI=CRC_HC_Val_Neg$auc_CI,
                                  tag="HC vs CRC with negative weights"))
DT::datatable(CRC_HC_Val_performance)



CRC_HC_filename_Elastic <- paste0("../../Result/Biomarker/Logistic_model/CRC_HC_DEA_Elastic-", Sys.Date(), ".RDS")
saveRDS(CRC_HC_Elastic , file = CRC_HC_filename_Elastic , compress = TRUE)
```


### CRC vs non_CRC 
```{r, fig.width=12, fig.height=5}
CRC_nonCRC_Elastic <- build_Elastic(dataset=ExprSet,
                                testres=CRC_nonCRC_DEA,
                                group_name=subgrp[c(1:3)],
                                Pname="P.value",
                                Pval=0.05,
                                LogFC=0,
                                splitPro=0.5,
                                kfold=10,
                                times=5)

CRC_nonCRC_performance <- rbind(
    model_performance(rocobject=CRC_nonCRC_Elastic$roc$AllRoc,
                  auc_CI=CRC_nonCRC_Elastic$auc_CI$AllAucCI,
                  tag="HC vs CRC"),
    model_performance(rocobject=CRC_nonCRC_Elastic$roc$PosRoc,
                  auc_CI=CRC_nonCRC_Elastic$auc_CI$PosAucCI,
                  tag="HC vs CRC with positive weights"),
    model_performance(rocobject=CRC_nonCRC_Elastic$roc$NegRoc,
                  auc_CI=CRC_nonCRC_Elastic$auc_CI$NegAucCI,
                  tag="HC vs CRC with negative weights"))
DT::datatable(CRC_nonCRC_performance)


CRC_nonCRC_Val_all <- model_validation(dataset=ExprSet_Val,
                                   datmarker=CRC_nonCRC_DEA,
                                   datfit=CRC_nonCRC_Elastic$fit$AllFit,
                                   group_name=subgrp[c(1:3)],
                                   All=TRUE,
                                   Pname="P.value",
                                   Pval=0.05,
                                   LogFC=0)

CRC_nonCRC_Val_Pos <- model_validation(dataset=ExprSet_Val,
                                   datmarker=CRC_nonCRC_Elastic$marker$PosMarker$Protein,
                                   datfit=CRC_nonCRC_Elastic$fit$PosFit,
                                   group_name=subgrp[c(1:3)],
                                   All=FALSE)
CRC_nonCRC_Val_Neg <- model_validation(dataset=ExprSet_Val,
                                   datmarker=CRC_nonCRC_Elastic$marker$NegMarker$Protein,
                                   datfit=CRC_nonCRC_Elastic$fit$NegFit,
                                   group_name=subgrp[c(1:3)],
                                   All=FALSE)


CRC_nonCRC_Val_performance <- rbind(
                model_performance(rocobject=CRC_nonCRC_Val_all$roc,
                                  auc_CI=CRC_nonCRC_Val_all$auc_CI,
                                  tag="HC vs CRC"),
                model_performance(rocobject=CRC_nonCRC_Val_Pos$roc,
                                  auc_CI=CRC_nonCRC_Val_Pos$auc_CI,
                                  tag="HC vs CRC with positive weights"),
                model_performance(rocobject=CRC_nonCRC_Val_Neg$roc,
                                  auc_CI=CRC_nonCRC_Val_Neg$auc_CI,
                                  tag="HC vs CRC with negative weights"))
DT::datatable(CRC_nonCRC_Val_performance)



CRC_nonCRC_filename_Elastic <- paste0("../../Result/Biomarker/Logistic_model/CRC_nonCRC_DEA_Elastic-", Sys.Date(), ".RDS")
saveRDS(CRC_nonCRC_Elastic , file = CRC_nonCRC_filename_Elastic , compress = TRUE)
```



### systemic information
```{r}
sessionInfo()
```


### Reference

1. [An Introduction to glmnet](https://glmnet.stanford.edu/articles/glmnet.html)

2. [how to calculate AUC confidence interval](https://www.r-bloggers.com/2019/08/how-to-get-an-auc-confidence-interval/)

3. [Classification Accuracy in R: Difference Between Accuracy, Precision, Recall, Sensitivity and Specificity](https://boostedml.com/2019/05/classification-accuracy-in-r-difference-between-accuracy-precision-recall-sensitivity-and-specificity.html)

4. [coords: Coordinates of a ROC curve](https://www.rdocumentation.org/packages/pROC/versions/1.18.0/topics/coords)
